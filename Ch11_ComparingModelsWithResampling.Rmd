---
title: 'Notes on Ch 11: Comparing Models with Resampling'
author: "Norman Lim"
date: "2025-08-26"
output:
  pdf_document: default
  html_document: default
---

This chapter talks about how to compare two or more models to understand which one is best. 


### Creating multiple models with workflow sets

Splitting the dataset into training and testing sets:
```{r}
library(tidymodels)
tidymodels_prefer()
data(ames)

ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)

ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```


Creating three different model recipes:
```{r}
basic_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude + Longitude, data = ames_train) |> 
  step_log(Gr_Liv_Area, base = 10) |> 
  step_other(Neighborhood, threshold = 0.01) |> 
  step_dummy(all_nominal_predictors())

interaction_rec <- 
  basic_rec |> 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_"))

spline_rec <- 
  interaction_rec |> 
  step_ns(Latitude, Longitude, deg_free = 50)
```

Creating a workflow set:
```{r}
preproc <-
  list(
    basic = basic_rec,
    interact = interaction_rec,
    splines = spline_rec
  )

lm_models <- workflow_set(preproc, list(lm = linear_reg()), cross = FALSE)

lm_models
```

Splitting the training data into 10 folds in preparation for cross-validation:
```{r}
set.seed(1001)
ames_folds <- vfold_cv(ames_train, v = 10)
ames_folds
```


Setting the `control` arguments for resampling:
```{r}
keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)
```


Resampling each model using the **purrr*-like function called `workflow_map()`:
```{r}
lm_models <-
  lm_models |> workflow_map(
    "fit_resamples",
    # Options to workflow_map()
    seed = 1101, verbose = TRUE,
    # Options to fit_resamples()
    resamples = ames_folds, control = keep_pred
  )

lm_models
```


Collating the performance statistics of the models:
```{r}
collect_metrics(lm_models) |> 
  filter(.metric == "rmse")
```

Since the result is a tibble, it is possible to add other results to it by binding rows. 

Because the option `save_workflow` was set to `TRUE` in the control arguments when the random forest (previous chapter) model was resampled, it is possible to add it to the results above. 

Code for the random forest model from Chapter 10:
```{r}
rf_model <- 
  rand_forest(trees = 1000) |> 
  set_engine("ranger") |> 
  set_mode("regression")

rf_wflow <- 
  workflow() |> 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude + Longitude
  ) |> 
  add_model(rf_model)

set.seed(1003)
rf_res <- rf_wflow |> fit_resamples(resamples = ames_folds, control = keep_pred)

rf_res
```

Adding the random forest model to to the workflow set:
```{r}
four_models <- as_workflow_set(random_forest = rf_res) |> 
  bind_rows(lm_models)

four_models
```


Plotting the confidence intervals for each model in the workflow set using `autoplot()`:
```{r}
library(ggrepel)
autoplot(four_models, metric = "rsq") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")
```

In the plot of $R^2$ confidence intervals, we can see that the random forest model was the "best" among the four models. 


## Comparing resampled performance statistics

There are some resamples where performance across models tends to be low and others where it tends to be high. This is called *resample-to-resample* component of variation. 

Demonstration of resample-to-resample variation for the linear models and random forest models used above:
```{r}
rsq_indiv_estimates <- collect_metrics(four_models, summarize = FALSE) |> 
  filter(.metric == "rsq")

rsq_indiv_estimates

rsq_wider <- rsq_indiv_estimates |> 
  select(wflow_id, .estimate, id) |> 
  pivot_wider(id_cols = "id", names_from = "wflow_id", values_from = .estimate)

rsq_wider

corrr::correlate(rsq_wider |> select(-id), quiet = TRUE)
```

The correlations are high, and they indicate that across models, there are large within-resample correlations. 

Plotting the $R^2$ statistics for each resample for each model:
```{r}
rsq_indiv_estimates |> 
  mutate(wflow_id = reorder(wflow_id, .estimate)) |> 
  ggplot(aes(x = wflow_id, y = .estimate, group = id, color = id)) +
  geom_line(alpha = 0.5, linewidth = 1.25) +
  theme(legend.position = "none")
```

If the resample-to-resample effect was not real, there would not be any parallel lines. 
Running a statistical test for the correlations on `rsq_wider`:
```{r}
rsq_wider |> 
  with( cor.test(basic_lm, splines_lm)) |> 
  tidy() |> 
  select(estimate, starts_with("conf"))
```

This test shows the `estimate` of the correlation and the confidence intervals and that the within-resample correlation appears to be real.  


## Simple hypothesis testing methods

Comparing two models at a time using the differences in $R^2$ values as the outcome data in an ANOVA model:
```{r}
compare_lm <- 
  rsq_wider |> 
  mutate(difference = splines_lm - basic_lm)

compare_lm

lm(difference ~ 1, data = compare_lm) |> 
  tidy(conf.int = TRUE) |> 
  select(estimate, p.value, starts_with("conf"))
```

Applying a paired t-test to `rsq_wider`:
```{r}
rsq_wider |> 
  with(t.test(splines_lm, basic_lm, paired = TRUE)) |>
  tidy() |> 
  select(estimate, p.value, starts_with("conf"))
```

Note that the p-value indicates a statistically significant signal. This shows that the collection of spline terms for the longitude and latitude appear to have an effect (the null hypothesis here states the the variations are random). However, the difference in $R^2$ is estimated at 0.91%. If our practical effect size were 2%, we might not consider these terms worth including in the model. 


## Bayesian methods

This is a more general approach to making formal comparisons using random effects.  

The model used here is more complex than the ANOVA method, but the interpretation is more straightforward than the p-value approach used in ANOVA.  

The Bayesian linear model makes the following assumptions:  
- The residuals are assumed to be independent and follow a Gaussian distribution with zero mean and constant standard deviation.  
- Prior distribution specifications are required.  


From the given observed data and prior distribution specifications, the model parameters can be estimated. The combinations of the priors and the likelihood estimates give the final (aka posterior) distributions of the model.  


#### A random intercept model

A random intercept model will be used in the demonstration that follows. This is to ensure that the Bayesian ANOVA model adequately models the resamples.  

In this model, the prior distribution is assumed to be symmetric, such as a bell-shaped curve (a standard normal or t-distribution).  

Using the **tidyposterior** and **rstanarm** for model analysis:
```{r}
library(tidyposterior)
library(rstanarm)

# The rstanarm package creates copious amounts of output; those results are worth
# inspecting for potential issues. option `refresh = 0` can be used to eliminate the
# logging. 

rsq_anova <- 
  perf_mod(
    four_models,
    metric = "rsq",
    prior_intercept = rstanarm::student_t(df = 1),
    chains = 4,
    iter = 5000,
    seed = 1102
  )
```

The resulting object has information on the resampling process as well as the Stan object embedded within (in an element called `stan`). The **tidyposterior** packaged has a `tidy()` method that extracts these posterior distributions into a tibble:
```{r}
model_post <- 
  rsq_anova |> 
  # Take a random sample from the posterior distribution so set the seed again to be
  # reproducible.
  tidy(seed = 1103)

glimpse(model_post)
```

Visualizing the four posterior distributions: 
```{r}
model_post |> 
  mutate(model = forcats::fct_inorder(model)) |> 
  ggplot(aes(x = posterior)) +
  geom_histogram(bins = 50, color = "white", fill = "blue", alpha = 0.4) +
  facet_wrap(~ model, ncol = 1)
```

These histograms describe the estimated probability distributions of the mean $R^2$ value for each model.  

There is also a basic `autoplot()` method for the model results:
```{r}
autoplot(rsq_anova) +
  geom_text_repel(aes(label = workflow), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")
```

One wonderful aspect of using resampling with Bayesian models is that, once we have the posteriors for parameters, it is trivial to get the posterior distributions for combinations of the parameters.  

Comparing the two linear models:
```{r}
rqs_diff <- 
  contrast_models(rsq_anova,
                  list_1 = "splines_lm",
                  list_2 = "basic_lm",
                  seed = 1104)
```

Visualizing the comparison results:
```{r}
rqs_diff |> 
  as_tibble() |> 
  ggplot(aes(x = difference)) +
  geom_vline(xintercept = 0, lty =2) +
  geom_histogram(bins = 50, color = "white", fill = "red", alpha = 0.4)
```

The posterior shows that the center of the distribution is greater than zero -- indicating that the model with splines typically had larger values -- but does overlap with zero to a degree.  

Computing the mean of the distribution as well as the credible intervals using the `summary()` method:
```{r}
summary(rqs_diff) |> 
  select(-starts_with("pract"))
```

The `probability` column reflects the proportion of the posterior that is greater than zero. This is the probability that the positive difference is real.  

The value is not close to zero, providing a strong case for statistical significance, i.e., the idea that statistically the actual difference is not zero.  

We can also compute the probability of being practically significant. In Bayesian analysis, this is a ROPE estimate (Region of Practical Equivalence). This can be estimated using the `size` option to the `summary` function: 
```{r}
summary(rqs_diff, size = 0.02) |> 
  select(contrast, starts_with("pract"))
```

Using `autoplot()` to show the `pract_equiv` results that compare each workflow to the current best (the random forest model, in this case):
```{r}
autoplot(rsq_anova, type = "ROPE", size = 0.02) + 
  geom_text_repel(aes(label = workflow)) +
  theme(legend.position = "none")
```

#### The effect of the amount of resampling 

Visualizing the effect of the number of resamples on these types of Bayesian comparisons:
```r
ggplot(
  intervals,
  aes(x = resamples, y = mean)
) + 
  geom_path() +
  geom_robbon(aes(ymin = lower, ymax = upper), fill = "red", alpha = 0.1) +
  labs(x = "Number of Resamples (repeated 10-fold cross-validation)")
```



